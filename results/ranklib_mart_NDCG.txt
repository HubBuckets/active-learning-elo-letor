
[+] General Parameters:
LETOR 4.0 dataset: No
Training data:	../data/MQ2008/Fold1/train.txt
Test data:	../data/MQ2008/Fold1/test.txt
Validation data:	../data/MQ2008/Fold1/vali.txt
Ranking method:	MART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	ERR@10
Highest relevance label (to compute ERR): 4
Feature normalization: No
Model file: ../results/ranklib_mart_model

[+] MART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/MQ2008/Fold1/train.txt]: 0... Reading feature file [../data/MQ2008/Fold1/train.txt]... [Done.]
(471 ranked lists, 9630 entries read)
Reading feature file [../data/MQ2008/Fold1/vali.txt]: 0... Reading feature file [../data/MQ2008/Fold1/vali.txt]... [Done.]
(157 ranked lists, 2707 entries read)
Reading feature file [../data/MQ2008/Fold1/test.txt]: 0... Reading feature file [../data/MQ2008/Fold1/test.txt]... [Done.]
(156 ranked lists, 2874 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@10-T | NDCG@10-V |
---------------------------------
1       | 0.4911    | 0.5366    |
2       | 0.4951    | 0.5366    |
3       | 0.5016    | 0.5358    |
4       | 0.5032    | 0.5372    |
5       | 0.5062    | 0.5343    |
6       | 0.5054    | 0.5388    |
7       | 0.5089    | 0.5443    |
8       | 0.5095    | 0.5396    |
9       | 0.5123    | 0.5427    |
10      | 0.5145    | 0.5395    |
11      | 0.5134    | 0.545     |
12      | 0.5126    | 0.5457    |
13      | 0.516     | 0.5443    |
14      | 0.515     | 0.5471    |
15      | 0.5145    | 0.5508    |
16      | 0.5163    | 0.5466    |
17      | 0.5167    | 0.5468    |
18      | 0.5165    | 0.5471    |
19      | 0.5174    | 0.5459    |
20      | 0.5178    | 0.5445    |
21      | 0.5187    | 0.5441    |
22      | 0.5191    | 0.5446    |
23      | 0.5201    | 0.5452    |
24      | 0.5187    | 0.5457    |
25      | 0.5186    | 0.5453    |
26      | 0.5216    | 0.5429    |
27      | 0.521     | 0.5417    |
28      | 0.5213    | 0.5409    |
29      | 0.5215    | 0.5415    |
30      | 0.5251    | 0.5416    |
31      | 0.5239    | 0.541     |
32      | 0.5288    | 0.5425    |
33      | 0.529     | 0.544     |
34      | 0.5285    | 0.5447    |
35      | 0.5302    | 0.5429    |
36      | 0.5292    | 0.5408    |
37      | 0.5283    | 0.5417    |
38      | 0.53      | 0.5409    |
39      | 0.5308    | 0.541     |
40      | 0.5316    | 0.5414    |
41      | 0.5326    | 0.5401    |
42      | 0.5325    | 0.5414    |
43      | 0.5343    | 0.5423    |
44      | 0.5356    | 0.5431    |
45      | 0.5355    | 0.5427    |
46      | 0.5338    | 0.5436    |
47      | 0.5346    | 0.544     |
48      | 0.5356    | 0.5441    |
49      | 0.5349    | 0.5423    |
50      | 0.5364    | 0.5422    |
51      | 0.5378    | 0.5436    |
52      | 0.5384    | 0.5436    |
53      | 0.5379    | 0.5423    |
54      | 0.5388    | 0.5423    |
55      | 0.5392    | 0.5411    |
56      | 0.5406    | 0.5437    |
57      | 0.5399    | 0.5435    |
58      | 0.5414    | 0.5438    |
59      | 0.5422    | 0.5444    |
60      | 0.5416    | 0.5467    |
61      | 0.5416    | 0.5477    |
62      | 0.5416    | 0.5474    |
63      | 0.5434    | 0.5464    |
64      | 0.5433    | 0.5472    |
65      | 0.5442    | 0.5465    |
66      | 0.5449    | 0.5458    |
67      | 0.5442    | 0.5458    |
68      | 0.5442    | 0.5455    |
69      | 0.5447    | 0.5441    |
70      | 0.5454    | 0.5439    |
71      | 0.5459    | 0.5457    |
72      | 0.5453    | 0.5435    |
73      | 0.5457    | 0.542     |
74      | 0.5469    | 0.5448    |
75      | 0.5473    | 0.5422    |
76      | 0.5485    | 0.5413    |
77      | 0.5499    | 0.5414    |
78      | 0.5489    | 0.5434    |
79      | 0.5489    | 0.5432    |
80      | 0.5505    | 0.5439    |
81      | 0.5524    | 0.5388    |
82      | 0.5513    | 0.5391    |
83      | 0.5512    | 0.54      |
84      | 0.551     | 0.5446    |
85      | 0.5517    | 0.5435    |
86      | 0.5519    | 0.542     |
87      | 0.5533    | 0.5411    |
88      | 0.5531    | 0.5418    |
89      | 0.5537    | 0.5404    |
90      | 0.5549    | 0.541     |
91      | 0.5569    | 0.5422    |
92      | 0.559     | 0.5404    |
93      | 0.5576    | 0.5385    |
94      | 0.5596    | 0.5397    |
95      | 0.5595    | 0.5394    |
96      | 0.559     | 0.5387    |
97      | 0.5589    | 0.5407    |
98      | 0.5597    | 0.541     |
99      | 0.5595    | 0.5402    |
100     | 0.5591    | 0.5399    |
101     | 0.5591    | 0.5387    |
102     | 0.5593    | 0.5406    |
103     | 0.5595    | 0.5413    |
104     | 0.5609    | 0.5399    |
105     | 0.5611    | 0.5412    |
106     | 0.5633    | 0.5392    |
107     | 0.5634    | 0.5398    |
108     | 0.5623    | 0.5364    |
109     | 0.562     | 0.5381    |
110     | 0.5623    | 0.5378    |
111     | 0.5629    | 0.5364    |
112     | 0.562     | 0.5375    |
113     | 0.5629    | 0.5381    |
114     | 0.5621    | 0.5379    |
115     | 0.5621    | 0.5348    |
116     | 0.5618    | 0.536     |
---------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.5145
NDCG@10 on validation data: 0.5508
---------------------------------
ERR@10 on test data: 0.0986

Model saved to: ../results/ranklib_mart_model
